{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1 code and knn implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf .csv with euclidean distance:\n",
      "accuracies: [0.5588235294117647, 0.5441176470588235, 0.5, 0.5735294117647058, 0.6029411764705882]\n",
      "Average accuracy: 0.5558823529411765  Performance time: 1.781332015991211 s Total samples: 340\n",
      "iris .csv with euclidean distance:\n",
      "accuracies: [0.9666666666666667, 1.0, 0.9666666666666667, 0.9666666666666667, 0.9]\n",
      "Average accuracy: 0.9600000000000002  Performance time: 0.20313239097595215 s Total samples: 150\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math \n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "from sklearn import svm\n",
    "\n",
    "def readIrisCsv(): # This function is written for leaf.csv, because of order of features and labels\n",
    "    csvOpen = open('iris.csv')\n",
    "    csvReader = csv.reader(csvOpen) # using csv library to read iris.csv\n",
    "    dataset = []\n",
    "    for row in csvReader:\n",
    "        dataset.append(row) # to csvreader to list\n",
    "    random.shuffle(dataset) # shuffling list for k-fold-validation\n",
    "    dataset_features = []\n",
    "    dataset_labels = []\n",
    "    for data in dataset: # iterating over dataset\n",
    "        label = data.pop(len(data)-1) ## We are droping class and save it to label\n",
    "        dataset_labels.append(label) # saving label to labels\n",
    "        dataset_features.append(list(map(float,data))) # saving features but data is full of string, change to float list\n",
    "    return dataset_labels,dataset_features # returning \n",
    "\n",
    "def readLeafCsv(): # This function is written for leaf.csv, because of order of features and labels\n",
    "    csvOpen = open('leaf.csv')\n",
    "    csvReader = csv.reader(csvOpen) # using csv library to read iris.csv\n",
    "    dataset = []\n",
    "    for row in csvReader:\n",
    "        dataset.append(row) # to csvreader to list\n",
    "    random.shuffle(dataset) # shuffling list for k-fold-validation\n",
    "    dataset_features = []\n",
    "    dataset_labels = []\n",
    "    for data in dataset: # iterating over dataset\n",
    "        label = data.pop(0) ## We are droping class and save it to label\n",
    "        data.pop(0) ## We are droping speciman number, because it won't help\n",
    "        dataset_labels.append(label) # saving label to labels\n",
    "        dataset_features.append(list(map(float,data))) # saving features but data is full of string, change to float list\n",
    "    return dataset_labels,dataset_features # returning them\n",
    "\n",
    "def split(labels,features,n,k): # this function splits dataset using k-fold-cross-validation\n",
    "        train_labels = []\n",
    "        train_features = []\n",
    "        test_labels = []\n",
    "        test_features = []\n",
    "        \n",
    "        for i in range(0,len(labels)): # iterating dataset\n",
    "            # This condition works like this : goo.gl/images/WNkSSV n is our iteration number k is our splitting factor.\n",
    "            if i >= (len(labels)/k)*(n-1) and i < (len(labels)/k)*(n-1)+len(labels)/k: #splitting test data with n\n",
    "                test_labels.append(labels[i])                                          #nth test data is chosen\n",
    "                test_features.append(features[i])\n",
    "            else:\n",
    "                train_labels.append(labels[i])\n",
    "                train_features.append(features[i])\n",
    "                \n",
    "        return train_labels,train_features,test_labels,test_features # returning splitted datas\n",
    "    \n",
    "def euclideanDistance(start,end): # This function calculates euclidianDistances of 2 points(dimension of R)\n",
    "    if type(start) != type([]) or type(end) != type([]): # type check for one dimention\n",
    "        return abs(start-end)\n",
    "    if len(start) != len(end): # Points must be in same R\n",
    "        return -99999\n",
    "    insideofroot = 0.0\n",
    "    \n",
    "    for i in range(0, len(start)): # Iteraing dimention\n",
    "        insideofroot+=(start[i]-end[i])**2 # For each dimention we do calculation\n",
    "    return math.sqrt(insideofroot) # returning final distance\n",
    "\n",
    "def doVotes(votes): # This function does voting with given votes array which contains [distance class]\n",
    "    #print(votes)\n",
    "    vote_count = []\n",
    "    for i in range(0,len(votes)): #Iterating votes array\n",
    "        vote_count.append(0)\n",
    "        for ii in range(i,len(votes)):\n",
    "            if votes[i][1] == votes[ii][1]: # If there is a same class vote\n",
    "                vote_count[i]+=1 # Add one more vote to this class\n",
    "    max_class = votes[0][1]\n",
    "    max_vote = vote_count[0]\n",
    "    for i in range(0,len(votes)): # Find maximum voted class\n",
    "        if vote_count[i] > max_vote:\n",
    "            max_class = votes[i][1]\n",
    "    return max_class # returning class which is most repeatative class\n",
    "            \n",
    "\n",
    "def knn(labels,features,predict,method,k): # knn classifier with given train data set\n",
    "    distances_labels = []\n",
    "    for i in range(0,len(features)): # Iterating dataset\n",
    "        distance_label = []\n",
    "        if method == 'euclidean': # If method euclidean\n",
    "            distance_label.append(euclideanDistance(features[i],predict)) #calculate distance between predict and data manhattan\n",
    "        elif method == 'manhattan': # If method manhattan\n",
    "            distance_label.append(manhattanDistance(features[i],predict)) # calculate distance between predict and data manhattan\n",
    "        distance_label.append(labels[i]) # append class of this distance\n",
    "        distances_labels.append(distance_label) # save it to distances_labels array\n",
    "    \n",
    "    # After calculating all distances\n",
    "    sorted_distances_labels = sorted(distances_labels, key = lambda x: float(x[0])) # Sort this distances\n",
    "    votes = sorted_distances_labels[:k] # Find first k distance-class arrays\n",
    "    return doVotes(votes) # Do votes with them\n",
    "\n",
    "def findLabels(labels): # This function finds how many different labels is there. for confusion matrix\n",
    "    classes = []\n",
    "    for label in labels:\n",
    "        try:\n",
    "            classes.index(label)\n",
    "        except:\n",
    "            classes.append(label)\n",
    "    return classes\n",
    "\n",
    "    \n",
    "def testDataKnn(train_labels,train_features,test_labels,test_features,distance_method,classes): # Testing test data\n",
    "    confusion_matrix = [[0 for x in range(len(classes))] for y in range(len(classes))] # Initialing confusion_matrix\n",
    "    for i in range(0,len(test_labels)): # iterating test data\n",
    "        prediction = knn(train_labels,train_features,test_features[i],distance_method,5) # find predion\n",
    "        confusion_matrix[classes.index(prediction)][classes.index(test_labels[i])]+=1 # Filling confusion_matrix\n",
    "    \n",
    "    correct_predictions=0\n",
    "    total_predictions=0\n",
    "    for i in range(0,len(confusion_matrix)): # Iterating confusion_matrix to calculate accuracy\n",
    "        for j in range(0,len(confusion_matrix)):\n",
    "            if i==j: # Only predictions with same index numbers are the correct predictions\n",
    "                correct_predictions+=confusion_matrix[i][i]\n",
    "            total_predictions+=confusion_matrix[i][j] # Calculating all predictions for accuracy\n",
    "    return correct_predictions/total_predictions # find accuracy and return\n",
    "\n",
    "# part:1,2,3,4 dataset:\"leaf\",\"iris\" , k: k-fold-crossvalidation and knn number\n",
    "def hw1(part,dataset,k):\n",
    "    start=time.time() # start time\n",
    "    if dataset==\"iris\":\n",
    "        labels,features = readIrisCsv() # read iris.csv\n",
    "    elif dataset==\"leaf\":\n",
    "        labels,features = readLeafCsv() # read leaf.csv\n",
    "    else:\n",
    "        print(\"Please choose between iris and leaf\")\n",
    "        return -1\n",
    "    accuracies = []\n",
    "    result = []\n",
    "    classes = findLabels(labels)\n",
    "    for i in range(1,k+1): # do k-fold-validation\n",
    "        train_labels,train_features,test_labels,test_features = split(labels,features,i,k)\n",
    "        if part==3 or part==4:\n",
    "            result = testDataSvm(train_labels,train_features,test_labels,test_features,classes)\n",
    "        elif part==1:\n",
    "            result = testDataKnn(train_labels,train_features,test_labels,test_features,\"euclidean\",classes)\n",
    "        elif part==2:\n",
    "            result = testDataKnn(train_labels,train_features,test_labels,test_features,\"manhattan\",classes)\n",
    "        else:\n",
    "            print(\"This homework only has 4 parts\")\n",
    "            return -1\n",
    "        accuracies.append(result) # find accuracies\n",
    "    results = 0\n",
    "    for i in range(0,len(accuracies)): # iterating accuracies\n",
    "        results += accuracies[i] # add accuracies\n",
    "    accuracy = results/len(accuracies) # calculate average\n",
    "    end=time.time() # end time\n",
    "    if part==1:\n",
    "        print(dataset,\".csv with euclidean distance:\")\n",
    "    elif part==2:\n",
    "        print(dataset,\".csv with manhattan distance:\")\n",
    "    elif part==3:\n",
    "        print(dataset,\".csv with Svm Linear:\")\n",
    "    elif part==4:\n",
    "        print(dataset,\".csv with Svm Polynomial:\")\n",
    "\n",
    "    print(\"accuracies:\",accuracies)\n",
    "    print(\"Average accuracy:\",accuracy,\n",
    "          \" Performance time:\",end-start,\"s\",\"Total samples:\",len(labels))\n",
    "\n",
    "hw1(1,\"leaf\",5)\n",
    "hw1(1,\"iris\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see although we have much more samples in leaf.csv ,our accuracy is pretty low when we compare to iris.csv. Cause is there are only 3 classes in iris. So we can train it much more efficiently than leaf. Every class has at least has 20-25 samples in iris but in leaf every class has 7-8 samples. Difference between performance times is very big. The reason of this, leaf has 14 dimension but iris has 4. Calculating euclidean distance takes much more time than calculating iris datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: missing manhattanDistance implemantation is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf .csv with manhattan distance:\n",
      "accuracies: [0.5588235294117647, 0.5882352941176471, 0.6176470588235294, 0.7205882352941176, 0.6176470588235294]\n",
      "Average accuracy: 0.6205882352941177  Performance time: 1.1496827602386475 s Total samples: 340\n",
      "iris .csv with manhattan distance:\n",
      "accuracies: [0.8666666666666667, 1.0, 0.9666666666666667, 1.0, 1.0]\n",
      "Average accuracy: 0.9666666666666668  Performance time: 0.17537522315979004 s Total samples: 150\n"
     ]
    }
   ],
   "source": [
    "def manhattanDistance(start,end): # This function calculates euclidianDistances of 2 points(dimension of R)\n",
    "    if type(start) != type([]) or type(end) != type([]): # type check for one dimention\n",
    "        return abs(start-end)\n",
    "    if len(start) != len(end): # Points must be in same R\n",
    "        return -99999\n",
    "    \n",
    "    result = 0\n",
    "    for i in range(0, len(start)): # Iteraing dimention\n",
    "        result+=abs(start[i]-end[i]) # For each dimension we do calculation\n",
    "    return result # returning final distance\n",
    "\n",
    "hw1(2,\"leaf\",5)\n",
    "hw1(2,\"iris\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, manhattan distance is much more faster than euclidean distance. The reason of this, we took sqrt of summation of \n",
    "substructions in euclidean distance. But we lose some accuracy because manhattan distance is not complex as euclidean. This accuracy lose is not too big, if we want to know a prediction very fast we can use manhattan distance rather than euclidian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3 Svm code implemantations for linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf .csv with Svm Linear:\n",
      "accuracies: [0.6617647058823529, 0.6470588235294118, 0.75, 0.8088235294117647, 0.6764705882352942]\n",
      "Average accuracy: 0.7088235294117646  Performance time: 0.1405041217803955 s Total samples: 340\n",
      "iris .csv with Svm Linear:\n",
      "accuracies: [1.0, 1.0, 0.9333333333333333, 0.9333333333333333, 1.0]\n",
      "Average accuracy: 0.9733333333333334  Performance time: 0.01562643051147461 s Total samples: 150\n"
     ]
    }
   ],
   "source": [
    "def svmTrain(train_labels,train_features,classes):\n",
    "    X = np.array(train_features)\n",
    "    mapped_train_labels = [] # All classes mapped to 0,1,2,3,4...\n",
    "    for label in train_labels:\n",
    "        mapped_train_labels.append(classes.index(label))\n",
    "    y = mapped_train_labels\n",
    "    clf = svm.SVC(kernel='linear', C = 100)\n",
    "    clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "def testDataSvm(train_labels,train_features,test_labels,test_features,classes):\n",
    "    confusion_matrix = [[0 for x in range(len(classes))] for y in range(len(classes))] # Initialing confusion_matrix\n",
    "    trainedClf = svmTrain(train_labels,train_features,classes)\n",
    "    predictions = trainedClf.predict(test_features)\n",
    "    for i in range(0,len(test_labels)):\n",
    "        confusion_matrix[predictions[i]][classes.index(test_labels[i])]+=1 # Filling confusion_matrix\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for i in range(0,len(confusion_matrix)): # Iterating confusion_matrix to calculate accuracy\n",
    "        for j in range(0,len(confusion_matrix)):\n",
    "            if i==j: # Only predictions with same index numbers are the correct predictions\n",
    "                correct_predictions+=confusion_matrix[i][i]\n",
    "            total_predictions+=confusion_matrix[i][j] # Calculating all predictions for accuracy\n",
    "            \n",
    "    return correct_predictions/total_predictions # find accuracy and return\n",
    "\n",
    "\n",
    "hw1(3,\"leaf\",5)\n",
    "hw1(3,\"iris\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see Svm Linear works better than knn implementations. Both performance time and accuracy. I changed C 0.1 to 1000.\n",
    "While C is getting bigger leaf.csv's accuracy is increasing. But iris.csv's accuracy getting decreasing. C = 100 is a good parameter for these datasets.\n",
    "\n",
    "I couldnt figure out roc curves with multiple classes, but i do k-fold cross validation and used confusion matrixes anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part4 with polynomial svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf .csv with Svm Polynomial:\n",
      "accuracies: [0.5882352941176471, 0.6470588235294118, 0.6029411764705882, 0.6470588235294118, 0.4852941176470588]\n",
      "Average accuracy: 0.5941176470588235  Performance time: 0.14404535293579102 s Total samples: 340\n",
      "iris .csv with Svm Polynomial:\n",
      "accuracies: [0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9]\n",
      "Average accuracy: 0.9533333333333334  Performance time: 0.4468722343444824 s Total samples: 150\n"
     ]
    }
   ],
   "source": [
    "def svmTrain(train_labels,train_features,classes):\n",
    "    X = np.array(train_features)\n",
    "    mapped_train_labels = [] # All classes mapped to 0,1,2,3,4...\n",
    "    for label in train_labels:\n",
    "        mapped_train_labels.append(classes.index(label))\n",
    "    y = mapped_train_labels\n",
    "    clf = svm.SVC(kernel='poly', C = 100,degree=3)\n",
    "    clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "hw1(4,\"leaf\",5)\n",
    "hw1(4,\"iris\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see polynomial degree of 3 works worse than linear svm for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
